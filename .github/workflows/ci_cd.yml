#CI_CD Pipeline: The ci_cd.yml file is the master blueprint for the entire 
#MLOps workflow. It orchestrates all stages in a single continuous process.
#Phase Name,                    YAML Stage,           Purpose,                                                                                                       Primary Output
#1. Continuous Integration (CI)->Setup and Installation->Prepares the isolated runner environment by installing dependencies and creating the train_sentiment script via setup.py.,->Functional Python Environment
#2. Continuous Training (CT)->Run Model Training->"Executes the train_sentiment script to train the model, evaluate it, and log it persistently to MLflow.",  ->MLflow Model Artifact (The trained model)
#3. Continuous Delivery (CD)->Build and Push Docker Image,->"Packages the inference code (predict.py), its dependencies, and the serving configuration into an isolated, versioned artifact.",-> Docker Image (The production serving container)
#The ci_cd.yml file is responsible for the training (CI/CT) and for the creation of the deployment asset (CD).
#Think of it this way: That is an extremely crucial point to clarify! You've noticed a potential contradiction in the terminology.You are absolutely correct that the ci_cd.yml file is primarily focused on the Continuous Integration (CI) and Continuous Training/Delivery (CD) stages, starting with setup and installation (pip install -e .) and running the training (train_sentiment).The confusion arose because the ultimate goal of the entire pipeline, as defined in that file, is to produce a deployable asset used for Inference/Serving.Here is the corrected explanation of the two distinct roles within that single YAML file:The Two Roles of ci_cd.ymlThe ci_cd.yml file is the master blueprint for the entire MLOps workflow. It orchestrates all stages in a single continuous process.Phase NameYAML StagePurposePrimary Output1. Continuous Integration (CI)Setup and InstallationPrepares the isolated runner environment by installing dependencies and creating the train_sentiment script via setup.py.Functional Python Environment2. Continuous Training (CT)Run Model TrainingExecutes the train_sentiment script to train the model, evaluate it, and log it persistently to MLflow.MLflow Model Artifact (The trained model)3. Continuous Delivery (CD)Build and Push Docker ImagePackages the inference code (predict.py), its dependencies, and the serving configuration into an isolated, versioned artifact.Docker Image (The production serving container)Correcting the MisconceptionThe ci_cd.yml file is responsible for the training (CI/CT) and for the creation of the deployment asset (CD).The reason we focus so much on the Docker image is that the image itself is the package that contains the inference code. Once that image is deployed, it performs inference for the end-user.Think of it this way:CI/CT Steps: The factory floor where the product (the trained model) is made.CD Steps: The packaging line where the delivery box (the Docker image) is created to securely ship the product to the store (deployment environment).ShutterstockExploreThe ci_cd.yml defines both the factory floor and the packaging line in one seamless script.
  #CI/CT Steps: The factory floor where the product (the trained model) is made.
  #CD Steps: The packaging line where the delivery box (the Docker image) is created 
    #to securely ship the product to the store (deployment environment).

name: CI/CD Pipeline for Sentiment Analysis

on:
  push:
    branches:
      - main

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Repository Code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          
      # --- CI STAGE: Setup and Installation ---
      - name: Install Project and Dependencies
        run: pip install -e .

      # ðŸ’¥ NLTK FIX: Download resources to a fixed path and set the ENV variable.
      # This addresses the persistent LookupError.
      - name: Download NLTK Resources
        run: |
          # Set the environment variable for subsequent steps
          echo "NLTK_DATA=/home/runner/nltk_data" >> $GITHUB_ENV
          
          # Download ALL required packages into the fixed location
          python -c "import nltk; nltk.download(['stopwords', 'wordnet', 'punkt'], download_dir='/home/runner/nltk_data')"
          
      # ðŸ’¥ DATA FIX: Prepare data directory (removed redundant 'cp' command)
      # This assumes the 'twitter_training.csv' is committed inside the 'data/' folder.
      - name: Prepare Data Directory
        run: |
          # Ensures the data folder exists. We rely on Git checkout for the file.
          mkdir -p data 
          echo "Data directory prepared. File checked out by Git."

      # --- CT STAGE: Train Model ---
      # ðŸ’¥ FINAL NLTK FIX: Explicitly pass the NLTK_DATA environment variable to the training script.
      - name: Run Model Training and Log to MLflow
        env:
          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
          MLFLOW_TRACKING_USERNAME: ${{ secrets.MLFLOW_TRACKING_USERNAME }}
          MLFLOW_TRACKING_PASSWORD: ${{ secrets.MLFLOW_TRACKING_PASSWORD }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          # This guarantees the script finds its NLTK data using the explicit path
          NLTK_DATA=/home/runner/nltk_data train_sentiment

      # --- CD STAGE: Build and Deploy Artifact (Docker) ---
      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }} # Use the Access Token here

      - name: Build and Push Docker Image
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          # !! REPLACE 'yourdockerhubusername' with your actual username !!
          tags: ganjidocker/sentiment-analysis-service:latest